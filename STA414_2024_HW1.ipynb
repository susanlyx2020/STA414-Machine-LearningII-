{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susanlyx2020/STA414-Machine-LearningII-/blob/main/STA414_2024_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version history:\n",
        "V0"
      ],
      "metadata": {
        "id": "JfxQlwCthQGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Deadline**: Feb 4, at 23:59PM.\n",
        "- **Submission**: You need to submit your solutions through Crowdmark, including all your derivations, plots, and your code. You can produce the files however you like (e.g. LATEX, Microsoft Word, etc), as long as it is readable. Points will be deducted if we have a hard time reading your solutions or understanding the structure of your code.\n",
        "- **Collaboration policy**: After attempting the problems on an individual basis, you may discuss and work together on the assignment with up to two classmates. However, **you must write your own code and write up your own solutions individually and explicitly name any collaborators** at the top of the homework."
      ],
      "metadata": {
        "id": "VzXgv3YBhNTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 - Decision Theory"
      ],
      "metadata": {
        "id": "OD4xvOTW4RwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One successful use of probabilistic models is for building spam filters, which take in an email and take different actions depending on the likelihood that it’s spam.\n",
        "\n",
        "Imagine you are running an email service. You have a well-calibrated spam classifier that tells you the probability that a particular email is spam: $p(spam|email)$. You have five options for what to do with each email: You can list it as important email, show it to the user, put it in the other folder, put it in the spam folder, or delete it entirely.\n",
        "\n",
        "Depending on whether or not the email really is spam, the user will suffer a different amount  of wasted time for the different actions we can take, $L$(action, spam):\n",
        "\n",
        "Action   | Spam        | Not spam\n",
        "-------- | ----------- | -----------\n",
        "Important| 30         | 0\n",
        "Show     | 10          | 2\n",
        "Other | 1             | 5\n",
        "Spam   | 3           | 50\n",
        "Delete   | 0           | 100"
      ],
      "metadata": {
        "id": "qDDDoOny4SGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.1\n",
        "[3pts] Plot the expected wasted user time for each of the five possible actions, as a function of the probability of spam: $p(spam|email)$."
      ],
      "metadata": {
        "id": "rftkuSSP4TyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FFSGtrvn4Vud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = [[30, 0],[10, 2], [1, 5], [3, 50],[0, 100]]\n",
        "actions_names = ['Important','Show', 'Other', 'Spam', 'Delete']\n",
        "num_actions = len(losses)\n",
        "def expected_loss_of_action(prob_spam, action):\n",
        "    #TODO: Return expected loss over a Bernoulli random variable\n",
        "    # with mean prob_spam.\n",
        "    # Losses are given by the table above.\n",
        "\n",
        "prob_range = np.linspace(0., 1., num=600)\n",
        "\n",
        "# Make plot\n",
        "for action in range(num_actions):\n",
        "    plt.plot(prob_range, expected_loss_of_action(prob_range, action), label=actions_names[action])\n",
        "\n",
        "plt.xlabel('$p(spam|email)$')\n",
        "plt.ylabel('Expected loss of action')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "PHgL1Mpf4XTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.2\n",
        "[2pts] Write a function that computes the optimal action given the probability of spam."
      ],
      "metadata": {
        "id": "pJxQYxLg4aDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_action(prob_spam):\n",
        "    #TODO: return best action given the probability of spam.\n",
        "    #Hint: np.argmin might be helpful."
      ],
      "metadata": {
        "id": "mBkhLwou4etN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.3\n",
        "[4pts] Plot the expected loss of the optimal action as a function of the probability of spam.\n",
        "\n",
        "\n",
        "Color the line according to the optimal action for that probability of spam.\n"
      ],
      "metadata": {
        "id": "s9o3Ni184dLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_range = np.linspace(0., 1., num=600)\n",
        "optimal_losses = []\n",
        "optimal_actions = []\n",
        "for p in prob_range:\n",
        "    # TODO: Compute the optimal action and its expected loss for\n",
        "    # probability of spam given by p.\n",
        "\n",
        "plt.xlabel('p(spam|email)')\n",
        "plt.ylabel('Expected loss of optimal action')\n",
        "plt.plot(prob_range, optimal_losses)"
      ],
      "metadata": {
        "id": "6Jdw_KI94hqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.4\n",
        "[4pts] For exactly which range of the probabilities of an email being spam should we set it as important?\n",
        "\n",
        "And based on user feedback, they want less email to be marked as important. How to change $L$(action=important, spam=True) so that only if $p(spam|email)$ < 0.01, email would be marked as important?\n",
        "\n",
        "Find the exact answer by hand using algebra."
      ],
      "metadata": {
        "id": "VGBLGrpN42Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]"
      ],
      "metadata": {
        "id": "GCIM_RsR4284"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1kRdfM6ol0R"
      },
      "source": [
        "# Q2 - Naïve Bayes, A Generative Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je6H8FAKpqmF"
      },
      "source": [
        "![](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true)\n",
        "\n",
        "\n",
        "In this question, we'll fit a Bernoulli Naïve Bayes model to the fashion MNIST dataset, and use this model for making predictions and generating new images from the same distribution. Fasion MNIST is a dataset of 28x28 images of items of clothing.\n",
        "\n",
        "We represent each image by a vector $x^{(i)} \\in \\{0,1\\}^{D}$, where 0 and 1 represent white and black pixels respectively, and $D=784$. Each class label $c^{(i)}$ is a different item of clothing, which in the code is represented by a K=10-dimensional one-hot vector.\n",
        "\n",
        "The Bernoulli Naïve Bayes model parameterized by $\\theta$ and $\\pi$ defines the following joint probability of $x$ and $c$,\n",
        "$$p(x,c|\\theta,\\pi) = p(c|\\pi)p(x|c,\\theta) = p(c|\\pi)\\prod_{j=1}^{D}p(x_j|c,\\theta),$$\n",
        "where $x_j | c,\\theta \\sim \\operatorname{Bernoulli}(\\theta_{jc})$, i.e. $p(x_j | c,\\theta) = \\theta_{jc}^{x_j}(1-\\theta_{jc})^{1-x_j}$, and $c|\\pi$ follows a simple categorical distribution, i.e. $p(c|\\pi) = \\pi_c$.\n",
        "\n",
        "We begin by learning the parameters $\\theta$ and $\\pi$.\n",
        "\n",
        "First The following code will download and prepare the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k587bbiSvhB4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import array\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def download(url, filename):\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    out_file = os.path.join('data', filename)\n",
        "    if not os.path.isfile(out_file):\n",
        "        urlretrieve(url, out_file)\n",
        "\n",
        "\n",
        "def fashion_mnist():\n",
        "    base_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
        "\n",
        "    def parse_labels(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "    def parse_images(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "    for filename in ['train-images-idx3-ubyte.gz',\n",
        "                     'train-labels-idx1-ubyte.gz',\n",
        "                     't10k-images-idx3-ubyte.gz',\n",
        "                     't10k-labels-idx1-ubyte.gz']:\n",
        "        download(base_url + filename, filename)\n",
        "\n",
        "    train_images = parse_images('data/train-images-idx3-ubyte.gz')\n",
        "    train_labels = parse_labels('data/train-labels-idx1-ubyte.gz')\n",
        "    test_images = parse_images('data/t10k-images-idx3-ubyte.gz')\n",
        "    test_labels = parse_labels('data/t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # Remove the data point that cause log(0)\n",
        "    remove = (14926, 20348, 36487, 45128, 50945, 51163, 55023)\n",
        "    train_images = np.delete(train_images,remove, axis=0)\n",
        "    train_labels = np.delete(train_labels, remove, axis=0)\n",
        "    return train_images, train_labels, test_images[:1000], test_labels[:1000]\n",
        "\n",
        "\n",
        "def load_fashion_mnist():\n",
        "    partial_flatten = lambda x: np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n",
        "    one_hot = lambda x, k: np.array(x[:, None] == np.arange(k)[None, :], dtype=int)\n",
        "    train_images, train_labels, test_images, test_labels = fashion_mnist()\n",
        "    train_images = (partial_flatten(train_images) / 255.0).astype(float)\n",
        "    test_images = (partial_flatten(test_images) / 255.0).astype(float)\n",
        "    train_images_binarized = (train_images > 0.5).astype(float)\n",
        "    test_images_binarized = (test_images > 0.5).astype(float)\n",
        "    train_labels = one_hot(train_labels, 10)\n",
        "    test_labels = one_hot(test_labels, 10)\n",
        "    N_data = train_images.shape[0]\n",
        "\n",
        "    return N_data, train_images, train_images_binarized, train_labels, test_images, test_images_binarized, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgGhDuEBvuMI"
      },
      "source": [
        "## Q2.1\n",
        "[4pts] Derive the expression for the Maximum Likelihood Estimator (MLE) of $\\theta$ and $\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cISpi3BUOdEp"
      },
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTMSP01Sw-F5"
      },
      "source": [
        "## Q2.2\n",
        "[5pts] Using the MLE for this data, many entries of $\\theta$ will be estimated to be 0, which seems extreme. So we look for another estimation method.\n",
        "\n",
        "Assume the prior distribution of $\\theta$ is such that the entries are i.i.d. and drawn from $\\operatorname{Beta}(\\alpha,\\alpha)$. Derive the Maximum A Posteriori (MAP) estimator for $\\theta$ (it has a simple final form). You can return the MLE for $\\pi$ in your implementation. From now on, we will work with this estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3hcFf1Of82"
      },
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49Abi0uxeII"
      },
      "outputs": [],
      "source": [
        "def train_map_estimator(train_images, train_labels, alpha):\n",
        "    \"\"\" Inputs:\n",
        "        train_images (N_samples x N_features)\n",
        "        train_labels (N_samples x N_classes)\n",
        "        alpha (float)\n",
        "        Returns the MAP estimator theta_est (N_features x N_classes) and the MLE\n",
        "        estimator pi_est (N_classes)\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yO5yq0dyus4"
      },
      "source": [
        "## Q2.3\n",
        "a) [4pts] Derive an expression for the class log-likelihood $\\log p(c|x,\\theta,\\pi)$ for a single image. Then, complete the implementation of the following functions. Recall that our prediction rule is to choose the class that maximizes the above log-likelihood, and accuracy is defined as the fraction of samples that are correctly predicted.\n",
        "\n",
        "Report the average log-likelihood $\\frac{1}{N}\\sum_{i=1}^{N}\\log p(c^{(i)}|x^{(i)},\\hat{\\theta},\\hat{\\pi})$ (where $N$ is the number of samples) on the training test, as well the training and test errors. Use a value of $\\alpha = 2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36FW8dZpOhb7"
      },
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RZwBnVh0Zoo"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(images, theta, pi):\n",
        "    \"\"\" Inputs: images (N_samples x N_features), theta, pi\n",
        "        Returns the matrix 'log_like' of loglikehoods over the input images where\n",
        "        log_like[i,c] = log p (c |x^(i), theta, pi) using the estimators theta and pi.\n",
        "        log_like is a matrix of (N_samples x N_classes)\n",
        "    Note that log likelihood is not only for c^(i), it is for all possible c's.\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "def accuracy(log_like, labels):\n",
        "    \"\"\" Inputs: matrix of log likelihoods and 1-of-K labels (N_samples x N_classes)\n",
        "    Returns the accuracy based on predictions from log likelihood values\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "\n",
        "N_data, train_images, train_images_binarized, train_labels, test_images, test_images_binarized, test_labels = load_fashion_mnist()\n",
        "\n",
        "theta_est, pi_est = train_map_estimator(train_images_binarized, train_labels, alpha=2.)\n",
        "\n",
        "loglike_train = log_likelihood(train_images_binarized, theta_est, pi_est)\n",
        "avg_loglike = np.sum(loglike_train * train_labels) / N_data\n",
        "train_accuracy = accuracy(loglike_train, train_labels)\n",
        "loglike_test = log_likelihood(test_images_binarized, theta_est, pi_est)\n",
        "test_accuracy = accuracy(loglike_test, test_labels)\n",
        "\n",
        "print(f\"Average log-likelihood for MAP with alpha = 2 is {avg_loglike:.3f}\")\n",
        "print(f\"Training accuracy for MAP with alpha = 2 is {train_accuracy:.3f}\")\n",
        "print(f\"Test accuracy for MAP with alpha = 2 is {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K27wtY9mwuTz"
      },
      "source": [
        "b) [2pts] Now compute the MAP estimators using $\\alpha = 1$. Then rerun the code for computing the log-likelihoods and accuracy.\n",
        "\n",
        "What do you observe? - comment on whether it was important or not to use the MAP (with $\\alpha > 1$). Based on your previous derivation, what does $\\alpha = 1$ correspond to?\n",
        "\n",
        "(Note: You do not need to report the average log-likelihoods or accuracy in this part.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ_kkyemwuTz"
      },
      "outputs": [],
      "source": [
        "theta_alpha1_est, pi_est = train_map_estimator(train_images_binarized, train_labels, alpha=1.)\n",
        "\n",
        "loglike_train = log_likelihood(train_images_binarized, theta_alpha1_est, pi_est)\n",
        "avg_loglike = np.sum(loglike_train * train_labels) / N_data\n",
        "train_accuracy = accuracy(loglike_train, train_labels)\n",
        "loglike_test = log_likelihood(test_images_binarized, theta_alpha1_est, pi_est)\n",
        "test_accuracy = accuracy(loglike_test, test_labels)\n",
        "\n",
        "print(f\"Average log-likelihood for MAP with alpha = 1 is {avg_loglike:.3f}\")\n",
        "print(f\"Training accuracy for MAP with alpha = 1 is {train_accuracy:.3f}\")\n",
        "print(f\"Test accuracy for MAP with alpha = 1 is {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKkS4A3hwuTz"
      },
      "source": [
        "[Type up your answer here.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFJkXeMK2mwP"
      },
      "source": [
        "## Q2.4\n",
        "[2pts] Given this model's assumptions, is it always true that any two pixels $x_i$ and $x_j$ with $i \\neq j$ are independent:\n",
        "- when conditioned on $c$?\n",
        "- after marginalizing over $c$?\n",
        "- when unconditioned on $c$?\n",
        "\n",
        "Provide brief justification for your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImntAmpWOjYe"
      },
      "source": [
        "[Type up your answer here]\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.5\n",
        "[3pts]\n",
        "- How many parameters need to be estimated in the Bernoulli Naive Bayes model?\n",
        "- How many parameters need to be estimated, if we remove the Naive Bayes assumption?\n",
        "\n",
        "Briefly justify your answers."
      ],
      "metadata": {
        "id": "wSYE3Gd8tdyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer here]\n",
        "\n",
        "Your answer:"
      ],
      "metadata": {
        "id": "_9YFdtnwtd9R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P4Y1x_G28QD"
      },
      "source": [
        "## Q2.6\n",
        "[4pts] Since we have a generative model for our data, we can do more than just prediction. Randomly sample and plot 10 images from the learned distribution using the MAP estimates. (Hint: You first need to sample the class $c$, and then sample pixels conditioned on $c$.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amV0qFMC3myy"
      },
      "outputs": [],
      "source": [
        "def image_sampler(theta, pi, num_images):\n",
        "    \"\"\" Inputs: parameters theta and pi, and number of images to sample\n",
        "    Returns the sampled images (N_images x N_features)\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "\n",
        "def plot_images(images, ims_per_row=5, padding=5, image_dimensions=(28, 28),\n",
        "                cmap=matplotlib.cm.binary, vmin=0., vmax=1.):\n",
        "    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n",
        "    fig = plt.figure(1)\n",
        "    fig.clf()\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    N_images = images.shape[0]\n",
        "    N_rows = np.int32(np.ceil(float(N_images) / ims_per_row))\n",
        "    pad_value = vmin\n",
        "    concat_images = np.full(((image_dimensions[0] + padding) * N_rows + padding,\n",
        "                             (image_dimensions[1] + padding) * ims_per_row + padding), pad_value)\n",
        "    for i in range(N_images):\n",
        "        cur_image = np.reshape(images[i, :], image_dimensions)\n",
        "        row_ix = i // ims_per_row\n",
        "        col_ix = i % ims_per_row\n",
        "        row_start = padding + (padding + image_dimensions[0]) * row_ix\n",
        "        col_start = padding + (padding + image_dimensions[1]) * col_ix\n",
        "        concat_images[row_start: row_start + image_dimensions[0],\n",
        "                      col_start: col_start + image_dimensions[1]] = cur_image\n",
        "        cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "\n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "sampled_images = image_sampler(theta_est, pi_est, 10)\n",
        "plot_images(sampled_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCYQ6VVx5YI5"
      },
      "source": [
        "## Q2.7\n",
        "[5pts] One of the advantages of generative models is that they can handle missing data, or be used to answer different sorts of questions about the model. Assume we have only observed some pixels of the image. Let $x_E = \\left\\{x_p : \\text{pixel p is observed}\\right\\}$. Derive an expression for $p(x_j|x_E,\\theta,\\pi)$, the conditional probability of an unobserved pixel $j$ given the observed pixels and distribution parameters. (Hint: You have to marginalize over $c$.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGT2yAtLOmJW"
      },
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qGf4Rwx6ZPy"
      },
      "source": [
        "## Q2.8\n",
        "a) [5pts] We assume that only 30% of the pixels are observed. For the first 30 images in the training set, plot the images when the unobserved pixels are left as white, as well as the same images when the unobserved pixels are filled with the marginal probability of the pixel being 1 given the observed pixels, i.e. the value of the unobserved pixel $j$ is $p(x_j = 1|x_E,\\theta,\\pi)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8g40hvw6pE_"
      },
      "outputs": [],
      "source": [
        "def probabilistic_imputer(theta, pi, original_images, is_observed):\n",
        "    \"\"\"Inputs: parameters theta and pi, original_images (N_images x N_features),\n",
        "        and is_observed which has the same shape as original_images, with a value\n",
        "        1. in every observed entry and 0. in every unobserved entry.\n",
        "    Returns the new images where unobserved pixels are replaced by their\n",
        "    conditional probability\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "\n",
        "num_features = train_images_binarized.shape[1]\n",
        "is_observed = np.random.binomial(1, p=0.3, size=(20, num_features))\n",
        "plot_images(train_images_binarized[:20] * is_observed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtOM8Ba4uVQj"
      },
      "outputs": [],
      "source": [
        "imputed_images = probabilistic_imputer(theta_est, pi_est, train_images_binarized[:20], is_observed)\n",
        "plot_images(imputed_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W63xAENTwuT0"
      },
      "source": [
        "b) [2pt] Now suppose instead of choosing the 30% observed pixels at random, we constructed a grid with roughly evenly spaced observed pixels, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmXpSHVDwuT1"
      },
      "outputs": [],
      "source": [
        "h, w = 28, 28\n",
        "p = 0.3\n",
        "\n",
        "num_pixels = h * w\n",
        "num_indices = int(num_pixels * p)\n",
        "\n",
        "step_size = num_pixels // num_indices\n",
        "\n",
        "indices = [(i // w, i % w) for i in range(0, num_pixels, step_size)]\n",
        "flattened_indices = [row * w + col for row, col in indices]\n",
        "\n",
        "one_hot_indices = np.zeros(num_pixels, dtype=int)\n",
        "for index in flattened_indices:\n",
        "    one_hot_indices[index] = 1\n",
        "\n",
        "one_hot_matrix = one_hot_indices.reshape(h, w)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(one_hot_matrix, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4L9jzBwuT1"
      },
      "source": [
        "Next we impute as before, except using this grid of observed indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm_0VuRtwuT1"
      },
      "outputs": [],
      "source": [
        "repeated_one_hot_indices = np.tile(one_hot_indices, (20, 1))\n",
        "imputed_images = probabilistic_imputer(theta_est, pi_est, train_images_binarized[:20], repeated_one_hot_indices)\n",
        "plot_images(imputed_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yIS8qxKwuT1"
      },
      "source": [
        "Compare the resulting images you found when using the probabilistic_imputer in part 2.8 a) (random 30% observed) and 2.8 (evenly spaced 30% observed).\n",
        "\n",
        "What do you find? Why might one of the two resulting samples of images be better than the other?\n",
        "(Hint: consider what the spatial properties of naturally occuring images are.)\\\n",
        "You can use the following ground truth images to help inform your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rcF7hWrwuT1"
      },
      "outputs": [],
      "source": [
        "plot_images(train_images[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer:"
      ],
      "metadata": {
        "id": "8nwR0VeewQGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmVloLPcwuT1"
      },
      "source": [
        "## Q2.9\n",
        "\n",
        "We now consider the Gaussian Naïve Bayes model to model the pixel data. Thus we revert back to representing each data sample using a continuous range of values, i.e. $x^{(i)} \\in \\mathbb{R}^{D}$, and we no longer use the binarized version of the data samples. (Note that in reality $x^{(i)} \\in \\left[0,1\\right]^{D}$, but for the purposes of this question we will ignore this.) Here the joint probability distribution of $x$ and $c$ is given by $$p(x,c|\\mu,\\Sigma,\\pi) =  p(c|\\pi)p(x|c,\\mu,\\Sigma) = p(c|\\pi)p(x|c,\\mu,\\Sigma),$$ where $p(x | c,\\mu,\\Sigma) = \\frac{1}{\\sqrt{\\left(2\\pi\\right)^{D}|\\Sigma_{c}|}}\\exp\\left(-\\frac{1}{2}\\left(x-\\mu_{c}\\right)^{T}\\Sigma_{c}^{-1}\\left(x-\\mu_{c}\\right)\\right)$. As before $c|\\pi$ follows a simple categorical distribution, i.e. $p(c|\\pi) = \\pi_c$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUR9QgYDwuT1"
      },
      "source": [
        "[1pts] Recall in general for the normal distribution, $\\Sigma_{c} \\in \\mathbb{R}^{D,D}$. What special form does $\\Sigma_{c}$ take, in context of the Naive Bayes assumption?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEgZdYXEwuT1"
      },
      "source": [
        "[Type your answer here.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjfQRTF2wuT2"
      },
      "source": [
        "[7pts] Derive the maximum likelihood estimates of $\\mu_{c}$, $\\Sigma_{c},\\pi_{c}$ for all $c \\in \\left\\{1, \\ldots, 10\\right\\}$, then fill in the code block below with the implementation.\\\n",
        "(Hint: it is normal if you use a single for loop over the computation of each $\\Sigma_{c}$)\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Togny1ZTwuT2"
      },
      "source": [
        "Derivation:\n",
        "\n",
        "Type up your answer here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSki9HWXwuT2"
      },
      "outputs": [],
      "source": [
        "def train_gnb_mle_estimator(train_images, train_labels, epsilon=1e-6):\n",
        "    \"\"\" Inputs:\n",
        "        train_images (N_samples x N_features)\n",
        "        train_labels (N_samples x N_classes)\n",
        "        Returns the MLE estimators mu_est, sigma_est, pi_est.\n",
        "    \"\"\"\n",
        "    # YOU NEED TO WRITE THIS PART\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7OF7pldwuT2"
      },
      "source": [
        "## Q2.10\n",
        "[6pts] Similar to before, derive an expression for the class log-likelihood $\\log p(c|x,\\mu,\\Sigma,\\pi)$ for a single image. Then, complete the implementation for computing the log_likelihoods.\n",
        "\n",
        "As before report the average log-likelihood $\\frac{1}{N}\\sum_{i=1}^{N}\\log p(c^{(i)}|x^{(i)},\\hat{\\theta},\\hat{\\pi})$ (where $N$ is the number of samples) on the training test, as well the training and test errors. Use the accuracy function you implemented earlier.\n",
        "\n",
        "Note: Here because we did not find the MAP estimators for $\\mu, \\Sigma$ we use a technique called variance smoothing, which adds a small value $\\epsilon$ to $\\Sigma$ in the implementation, before computing the log-likelihoods. The subsequent part of this question comments on what one might expect if they used the MAP estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:"
      ],
      "metadata": {
        "id": "qMPHc_MPxDqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrGOkq9rwuT2"
      },
      "outputs": [],
      "source": [
        "def gnb_log_likelihood(images, mu, sigma, pi, epsilon):\n",
        "    sigma = sigma + epsilon\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "\n",
        "N_data, train_images, _, train_labels, test_images, _, test_labels = load_fashion_mnist()\n",
        "mu_est, sigma_est, pi_est = train_gnb_mle_estimator(train_images, train_labels)\n",
        "\n",
        "epsilon = 1e-05\n",
        "loglike_train = gnb_log_likelihood(train_images, mu_est, sigma_est, pi_est, epsilon)\n",
        "avg_loglike = np.sum(loglike_train * train_labels) / N_data\n",
        "train_accuracy = accuracy(loglike_train, train_labels)\n",
        "loglike_test = gnb_log_likelihood(test_images, mu_est, sigma_est, pi_est, epsilon)\n",
        "test_accuracy = accuracy(loglike_test, test_labels)\n",
        "\n",
        "print(f\"Average log-likelihood for MLE (with variance smoothing) is {avg_loglike:.3f}\")\n",
        "print(f\"Training accuracy for MLE (with variance smoothing) is {train_accuracy:.3f}\")\n",
        "print(f\"Test accuracy for MLE (with variance smoothing) is {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DZxwFUHwuT2"
      },
      "source": [
        "Using a small value of $\\epsilon$ is a crude way of circumventing the problem of using the MLE directly.\n",
        "\n",
        "The MAP for the Gaussian Naive Bayes can be obtained with the help of the conjugate prior for the multivariate normal distribution, which is the [normal-inverse-Wishart](https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution). We will not derive the MAP for the Gaussian Naive Bayes here - but one can still appreciate the benefit of doing so by drawing a comparison to what was found for the Bernoulli Naive Bayes, when $\\alpha=2$ was used as compared with $\\alpha=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Bayes Ball Algorithm"
      ],
      "metadata": {
        "id": "Pibgnd1k6cLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.1\n",
        "[8pts]\n",
        "\n",
        "Consider the following directed acyclic graph. Use the Bayes ball algorithm to compute all the nodes that are independent of $C$ given\n",
        "\n",
        "i: $\\{B, D\\}$,\n",
        "\n",
        "ii: $\\{H\\}$.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1bayO6U6K5S2hfFvUFxu4ceu11xS6UQug)\n",
        "\n"
      ],
      "metadata": {
        "id": "AaLdIw6b6hPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer below]\n",
        "\n"
      ],
      "metadata": {
        "id": "w_C5SkE16miz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.2\n",
        "[6pts] For the graph shown above, show using the factorization of the joint probabilities whether $D$ is independent of $H$ given $E$. You may suppose that the domain of the variable $G$ is $\\mathbf{G}$.\n",
        "\n",
        "Hint:  $P(E|D) = \\sum_{G \\in \\mathbf{G}} P(E|D,G) P(G) $."
      ],
      "metadata": {
        "id": "MuZG--jb6nBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer below]"
      ],
      "metadata": {
        "id": "fFgSVX1H6psQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.3\n",
        "\n",
        "[6pts] Consider the following lattice structure with the diagonal nodes shaded. You may assume that it extends arbitrarily far upwards and also to the right.\n",
        "\n",
        "Conditioned on the shaded nodes, what are the set of all nodes independent of $C_2$? Justify your answer.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=16m-VS7u4fVYmlYc-kOhNt5tusFed7ns1)"
      ],
      "metadata": {
        "id": "azPSyyTT6ugU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer below]"
      ],
      "metadata": {
        "id": "ZqCZqWGr6xU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.4\n",
        "\n",
        "[7pts] Consider the following checkboard variant of the lattice from the previous question. Assume it extends indefinitely to the right and upwards (not shown in the diagram).\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=19vPIisi7zEP8yFS4S3lnOMDKGWSDZqQA)\n",
        "\n",
        "Condition on the shaded nodes. What are the set of all nodes that are conditionally independent of the variable $B_3$? (You may also state the set conditionally dependent on $B_3$ if it is easier).\n",
        "\n",
        "As a result, what can be concluded about the \"shape\" of the set of conditionally independent nodes for an arbitrary unshaded node?"
      ],
      "metadata": {
        "id": "Kj2qn5hS6zWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer below]"
      ],
      "metadata": {
        "id": "EHjswy1k7HnP"
      }
    }
  ]
}