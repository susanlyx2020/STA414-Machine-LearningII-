{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c0pDDwPK1Ijw",
        "MRBhMjym577V"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TrueSkill Model\n",
        "Implementing a variant of the [TrueSkill](http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf) model, a player ranking system for competitive games originally developed for Halo 2. It is a generalization of the Elo rating system in Chess."
      ],
      "metadata": {
        "id": "HPQVDRA1zYrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import wget\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "import torch\n",
        "import random\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Q8y6eQ3ozk9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Examining the posterior for only two players and toy data"
      ],
      "metadata": {
        "id": "c0pDDwPK1Ijw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#computes the log of the prior, jointly evaluated over all player's skills.\n",
        "def log_joint_prior(zs_array):\n",
        "  log_prob = torch.sum(Normal(0, 1).log_prob(zs_array))\n",
        "  return log_prob\n",
        "\n",
        "#evaluates the log-likelihood that player with skill z_a beat player with skill z_b\n",
        "def logp_a_beats_b(z_a, z_b):\n",
        "  logp_a = -torch.logaddexp(torch.tensor(0.0), (z_b-z_a))\n",
        "  return logp_a\n",
        "\n",
        "def logp_b_beats_a(z_a, z_b):\n",
        "  logp_b = -torch.logaddexp(torch.tensor(0.0), (z_a-z_b))\n",
        "  return logp_b\n",
        "\n",
        "def plot_isocontours(ax, func, steps=100):\n",
        "    x = torch.linspace(-4, 4, steps=steps)\n",
        "    y = torch.linspace(-4, 4, steps=steps)\n",
        "    X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
        "    Z = func(X, Y)\n",
        "    cs = plt.contour(X, Y, Z )\n",
        "    plt.clabel(cs, inline=1, fontsize=10)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "\n",
        "def plot_2d_fun(f, x_axis_label=\"\", y_axis_label=\"\", scatter_pts=None):\n",
        "    fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "    ax = fig.add_subplot(111, frameon=False)\n",
        "    ax.set_xlabel(x_axis_label)\n",
        "    ax.set_ylabel(y_axis_label)\n",
        "    plot_isocontours(ax, f)\n",
        "    if scatter_pts is not None:\n",
        "      plt.scatter(scatter_pts[:,0], scatter_pts[:, 1])\n",
        "    plt.plot([4, -4], [4, -4], 'b--')   # Line of equal skill\n",
        "    plt.show(block=True)\n",
        "    plt.draw()\n",
        "\n",
        " # plot the isocontours of the joint prior over their skills\n",
        "def log_prior_over_2_players(z1, z2):\n",
        "\n",
        "  prior2 = Normal(0,1).log_prob(z1) + Normal(0,1).log_prob(z2)\n",
        "  return prior2\n",
        "\n",
        "def prior_over_2_players(z1, z2):\n",
        "  return torch.exp(log_prior_over_2_players(z1, z2))\n",
        "\n",
        "plot_2d_fun(prior_over_2_players, \"Player A Skill\", \"Player B Skill\")\n",
        "\n",
        "\n",
        "def likelihood_over_2_players(z1, z2):\n",
        "  return torch.exp(logp_a_beats_b(z1, z2))\n",
        "\n",
        "plot_2d_fun(likelihood_over_2_players, \"Player A Skill\", \"Player B Skill\")\n",
        "\n",
        "# Plot isocountours of the joint posterior over z_A and z_B given that player A beat player B in one match\n",
        "def log_posterior_A_beat_B(z1, z2):\n",
        "  logp = log_prior_over_2_players(z1, z2) + logp_a_beats_b(z1, z2)\n",
        "  return logp\n",
        "\n",
        "def posterior_A_beat_B(z1, z2):\n",
        "  post = torch.exp(log_posterior_A_beat_B(z1, z2))\n",
        "  return post\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B, \"Player A Skill\", \"Player B Skill\")\n",
        "\n",
        "#plot isocountours of the joint posterior over $z_A$ and $z_B$ given that 5 matches were played, and player A beat player B in all matches\n",
        "def log_posterior_A_beat_B_5_times(z1, z2):\n",
        "  return log_prior_over_2_players(z1,z2) + 5 * logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B_5_times(z1, z2):\n",
        "  return torch.exp(log_posterior_A_beat_B_5_times(z1, z2))\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B_5_times, \"Player A Skill\", \"Player B Skill\")\n",
        "\n",
        "\n",
        "# Plot isocontours of the joint posterior over z_A and z_B given that 10 matches were played, and each player beat the other 5 times.\n",
        "def log_posterior_beat_each_other_5_times(z1, z2):\n",
        "  return log_prior_over_2_players(z1,z2) + 5 * logp_a_beats_b(z1, z2) + 5 * logp_b_beats_a(z1, z2)\n",
        "\n",
        "def posterior_beat_each_other_5_times(z1, z2):\n",
        "  return torch.exp(log_posterior_beat_each_other_5_times(z1, z2))\n",
        "\n",
        "plot_2d_fun(posterior_beat_each_other_5_times, \"Player A Skill\", \"Player B Skill\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nwpVfZAw2kfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Approximate posterior distributions with gradient-based Hamiltonian Monte Carlo.\n"
      ],
      "metadata": {
        "id": "zQZfqwiT25I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "# Hamiltonian Monte Carlo\n",
        "from tqdm import trange, tqdm_notebook  # Progress meters\n",
        "\n",
        "def leapfrog(params_t0, momentum_t0, stepsize, logprob_grad_fun):\n",
        "  # Performs a reversible update of parameters and momentum\n",
        "  # See https://en.wikipedia.org/wiki/Leapfrog_integration\n",
        "  momentum_thalf = momentum_t0    + 0.5 * stepsize * logprob_grad_fun(params_t0)\n",
        "  params_t1 =      params_t0      +       stepsize * momentum_thalf\n",
        "  momentum_t1 =    momentum_thalf + 0.5 * stepsize * logprob_grad_fun(params_t1)\n",
        "  return params_t1, momentum_t1\n",
        "\n",
        "def iterate_leapfrogs(theta, v, stepsize, num_leapfrog_steps, grad_fun):\n",
        "  for i in range(0, num_leapfrog_steps):\n",
        "    theta, v = leapfrog(theta, v, stepsize, grad_fun)\n",
        "  return theta, v\n",
        "\n",
        "def metropolis_hastings(state1, state2, log_posterior):\n",
        "  # Compares the log_posterior at two values of parameters,\n",
        "  # and accepts the new values proportional to the ratio of the posterior\n",
        "  # probabilities.\n",
        "  accept_prob = torch.exp(log_posterior(state2) - log_posterior(state1))\n",
        "  if random.random() < accept_prob:\n",
        "    return state2  # Accept\n",
        "  else:\n",
        "    return state1  # Reject\n",
        "\n",
        "def draw_samples(num_params, stepsize, num_leapfrog_steps, n_samples, log_posterior):\n",
        "  theta = torch.zeros(num_params)\n",
        "\n",
        "  def log_joint_density_over_params_and_momentum(state):\n",
        "    params, momentum = state\n",
        "    m = Normal(0., 1.)\n",
        "    return m.log_prob(momentum).sum(axis=-1) + log_posterior(params)\n",
        "\n",
        "  def grad_fun(zs):\n",
        "    zs = zs.detach().clone()\n",
        "    zs.requires_grad_(True)\n",
        "    y = log_posterior(zs)\n",
        "    y.backward()\n",
        "    return zs.grad\n",
        "\n",
        "\n",
        "  sampleslist = []\n",
        "  for i in trange(0, n_samples):\n",
        "    sampleslist.append(theta)\n",
        "\n",
        "    momentum = torch.normal(0, 1, size = np.shape(theta))\n",
        "\n",
        "    theta_new, momentum_new = iterate_leapfrogs(theta, momentum, stepsize, num_leapfrog_steps, grad_fun)\n",
        "\n",
        "    theta, momentum = metropolis_hastings((theta, momentum), (theta_new, momentum_new), log_joint_density_over_params_and_momentum)\n",
        "  return torch.stack((sampleslist))\n",
        "\n",
        "\n",
        "# approximate the joint posterior where we observe player A winning 1 game.\n",
        "num_players = 2\n",
        "num_leapfrog_steps = 20\n",
        "n_samples = 2500\n",
        "stepsize = 0.01\n",
        "\n",
        "def log_posterior_a(zs):\n",
        "  z1, z2 = zs[0], zs[1]\n",
        "  return log_posterior_A_beat_B(z1, z2)\n",
        "\n",
        "samples_a = draw_samples(num_players, stepsize, num_leapfrog_steps, n_samples, log_posterior_a)\n",
        "plot_2d_fun(posterior_A_beat_B, \"Player A Skill\", \"Player B Skill\", samples_a)\n",
        "\n",
        "# approximate the joint posterior where we observe player A winning 5 games against player B\n",
        "# Hyperparameters\n",
        "num_players = 2\n",
        "num_leapfrog_steps = 20\n",
        "n_samples = 2500\n",
        "stepsize = 0.01\n",
        "key = 42\n",
        "\n",
        "\n",
        "def log_posterior_b(zs):\n",
        "  z1 = zs[0]\n",
        "  z2 = zs[1]\n",
        "  return log_posterior_A_beat_B_5_times(z1, z2)\n",
        "\n",
        "samples_b = draw_samples(num_players, stepsize, num_leapfrog_steps, n_samples, log_posterior_b)\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B_5_times, \"Player A Skill\", \"Player B Skill\", samples_b)\n",
        "\n",
        "# approximate the joint posterior where we observe player A winning 5 games and player B winning 5 games.\n",
        "num_players = 2\n",
        "num_leapfrog_steps = 20\n",
        "n_samples = 2500\n",
        "stepsize = 0.01\n",
        "\n",
        "\n",
        "def log_posterior_c(zs):\n",
        "  z1 = zs[0]\n",
        "  z2 = zs[1]\n",
        "  return log_posterior_beat_each_other_5_times(z1,z2)\n",
        "\n",
        "samples_c = draw_samples(num_players, stepsize, num_leapfrog_steps, n_samples, log_posterior_c)\n",
        "plot_2d_fun(posterior_beat_each_other_5_times, \"Player A Skill\", \"Player B Skill\", samples_c)"
      ],
      "metadata": {
        "id": "1jDjV1tP3Bsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Variational Inference in the TrueSkill Model"
      ],
      "metadata": {
        "id": "Lixxl1W-5ZsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model definition\n",
        "\n",
        "We assume that each player has a true, but unknown skill $z_i \\in \\mathbb{R}$.\n",
        "We use $N$ to denote the number of players.\n",
        "\n",
        "### The prior:\n",
        "The prior over each player's skill is a standard normal distribution, and all player's skills are *a priori* independent.\n",
        "\n",
        "### The likelihood:\n",
        "For each observed game, the probability that player $A$ beats player $B$, given the player's skills $z_A$ and $z_B$, is:\n",
        "$$p(A \\,\\, \\text{beat} \\,\\, B | z_A, z_B) = \\sigma(z_A - z_B)$$\n",
        "where\n",
        "$$\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$$\n",
        "We chose this function simply because it's close to zero or one when the player's skills are very different, and equals one-half when the player skills are the same.  This likelihood function is the only thing that gives meaning to the latent skill variables $z_1$"
      ],
      "metadata": {
        "id": "MRBhMjym577V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import wget\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "import torch\n",
        "import random\n",
        "from torch import nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import trange, tqdm_notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function\n",
        "def diag_gaussian_log_density(x, mu, std):\n",
        "    # axis=-1 means sum over the last dimension.\n",
        "    m = Normal(mu, std)\n",
        "    return torch.sum(m.log_prob(x), axis=-1)"
      ],
      "metadata": {
        "id": "OTbCgzan6Bzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Implementing the Model\n"
      ],
      "metadata": {
        "id": "empJguWE6Gm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_joint_prior(zs_array):\n",
        "    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))\n",
        "\n",
        "def logp_a_beats_b(z_a, z_b):\n",
        "    return -torch.logaddexp(torch.tensor([0.0]), z_b - z_a)\n",
        "\n",
        "def log_prior_over_2_players(z1, z2):\n",
        "    m = Normal(torch.tensor([0.0]), torch.tensor([[1.0]]))\n",
        "    return m.log_prob(z1) + m.log_prob(z2)\n",
        "\n",
        "def prior_over_2_players(z1, z2):\n",
        "    return torch.exp(log_prior_over_2_players(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B_10_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + 10.0 * logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B_10_times(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B_10_times(z1, z2))\n",
        "\n",
        "def log_posterior_beat_each_other_10_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) \\\n",
        "        + 10.* logp_a_beats_b(z1, z2) \\\n",
        "        + 10.* logp_a_beats_b(z2, z1)\n",
        "\n",
        "def posterior_beat_each_other_10_times(z1, z2):\n",
        "    return torch.exp(log_posterior_beat_each_other_10_times(z1, z2))\n",
        "\n",
        "\n",
        "def plot_isocontours(ax, func, xlimits=[-4, 4], ylimits=[-4, 4], steps=101, cmap=\"summer\"):\n",
        "    x = torch.linspace(*xlimits, steps=steps)\n",
        "    y = torch.linspace(*ylimits, steps=steps)\n",
        "    X, Y = torch.meshgrid(x, y)\n",
        "    Z = func(X, Y)\n",
        "    plt.contour(X, Y, Z, cmap=cmap)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "\n",
        "def plot_2d_fun(f, x_axis_label=\"\", y_axis_label=\"\", f2=None, scatter_pts=None):\n",
        "    fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "    ax = fig.add_subplot(111, frameon=False)\n",
        "    ax.set_xlabel(x_axis_label)\n",
        "    ax.set_ylabel(y_axis_label)\n",
        "    plot_isocontours(ax, f)\n",
        "    if f2 is not None:\n",
        "      plot_isocontours(ax, f2, cmap='winter')\n",
        "\n",
        "    if scatter_pts is not None:\n",
        "      plt.scatter(scatter_pts[:,0], scatter_pts[:, 1])\n",
        "    plt.plot([4, -4], [4, -4], 'b--')   # Line of equal skill\n",
        "    plt.show(block=True)\n",
        "    plt.draw()"
      ],
      "metadata": {
        "id": "wcxNycBV6MM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Stochastic Variational Inference on Two Players and Toy Data\n",
        "The original Trueskill paper from 2007 used message passing.\n",
        "Here, I will approximate posterior distributions with gradient-based stochastic variational inference."
      ],
      "metadata": {
        "id": "X21yqXQp6auf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diag_gaussian_samples(mean, log_std, num_samples):\n",
        "    epsilon = torch.randn(num_samples, mean.shape[0])\n",
        "    samples = mean + torch.exp(log_std) * epsilon\n",
        "    return samples\n",
        "\n",
        "import math\n",
        "def diag_gaussian_logpdf(x, mean, log_std):\n",
        "    return torch.sum(Normal(mean, torch.exp(log_std)).log_prob(x), axis=-1)\n",
        "\n",
        "def batch_elbo(logprob, mean, log_std, num_samples):\n",
        "    samples = diag_gaussian_samples(mean, log_std, num_samples)\n",
        "    log_variational_density = diag_gaussian_logpdf(samples, mean, log_std)\n",
        "    log_density_samples = logprob(samples)\n",
        "    elbo_estimate = torch.mean(log_density_samples - log_variational_density)\n",
        "\n",
        "    return elbo_estimate\n",
        "\n",
        "# Hyperparameters\n",
        "num_players = 2\n",
        "n_iters = 800\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def log_posterior_A_beat_B_10_times_1_arg(z1z2):\n",
        "  return log_posterior_A_beat_B_10_times(z1z2[:,0], z1z2[:,1])\n",
        "\n",
        "def objective(params):  # The loss function to be minimized.\n",
        "  mean, log_std = params\n",
        "  return -batch_elbo(log_posterior_A_beat_B_10_times_1_arg, mean, log_std, num_samples_per_iter)"
      ],
      "metadata": {
        "id": "sWq6HVq97gbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a set of variational parameters and optimize them (written for you already) to approximate the joint distribution where we observe player A winning 10 games. Report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n"
      ],
      "metadata": {
        "id": "_VZ6EhdY789a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(params, t):\n",
        "  if t % 25 == 0:\n",
        "    print(\"Iteration {} lower bound {}\".format(t, objective(params)))\n",
        "\n",
        "# Set up optimizer.\n",
        "D = 2\n",
        "init_log_std  = torch.tensor([0.0, 0.0], requires_grad=True) # TODO.\n",
        "init_mean = torch.tensor([0.0, 0.0], requires_grad=True)# TODO\n",
        "\n",
        "params = [init_mean, init_log_std]\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Main loop.\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "\n",
        "def approx_posterior_2d(z1, z2):\n",
        "\n",
        "    mean, logstd = params[0].detach(), params[1].detach()\n",
        "    zs = torch.stack([z1.flatten(), z2.flatten()], dim=1)\n",
        "    probs = torch.exp(diag_gaussian_logpdf(zs, mean, logstd))\n",
        "    return probs.view(z1.shape)\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B_10_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_iters = 100\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def log_posterior_beat_each_other_10_times_1_arg(z1z2):\n",
        "    return log_posterior_A_beat_B_10_times(z1z2[:,0], z1z2[:,1])\n",
        "\n",
        "def objective(params):\n",
        "    mean, log_std = params\n",
        "\n",
        "    samples = diag_gaussian_samples(mean, log_std, num_samples_per_iter)\n",
        "    log_probs = log_posterior_beat_each_other_10_times_1_arg(samples)\n",
        "\n",
        "    return -torch.mean(log_probs)\n",
        "\n",
        "# Main loop.\n",
        "init_mean = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "init_log_std = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "plot_2d_fun(posterior_beat_each_other_10_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)"
      ],
      "metadata": {
        "id": "gtF56HTh8DTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}